# -*- coding: utf-8 -*-
"""pfizer_twitter_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HAV35wliYUzViS9H3bySIQLcO8hiNKXH

# Pfizer Vaccine Tweets Analysis
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Importing the packages"""

# Import the packages
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns

import requests
from bs4 import BeautifulSoup
import geopy
from geopy.geocoders import Nominatim
from geopy.extra.rate_limiter import RateLimiter
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator


import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, TweetTokenizer
from nltk.stem import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.sentiment import vader 
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

import warnings
warnings.filterwarnings("ignore")

# Read the data
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Pfizer Vaccine Tweets/vaccination_tweets.csv')
df.head()

"""## Exploratory Data Analysis"""

df.info()

print("Data is from")
print(df.date.min())
print("till")
print(df.date.max())

"""### How are people tweeting?"""

source_counts = df.groupby('source').count()[['id']].sort_values('id', ascending=False)
ax = plt.figure(figsize=(15, 8), facecolor='lightgrey')

source_counts_plot = sns.barplot(data=source_counts, x=source_counts.index, y='id')
plt.xticks(rotation=75)

for p in source_counts_plot.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height() + 50
  value = '{:.0f}'.format(p.get_height())
  source_counts_plot.text(_x, _y, value, ha="center") 

plt.grid()
plt.title("How are people tweeting?")
plt.xlabel("Source")
plt.ylabel("Frequency")
plt.show()

all_tweets = list(df.text)
stopwords_eng = nltk.corpus.stopwords.words('english')

all_words_raw = [word for tweet in all_tweets for word in tweet.split(' ')]
all_words = [word for word in all_words_raw if not word in stopwords_eng]

"""### Tweets about the 'Vaccine'"""

# Finding context around the term 'vaccine'
print("Context around the word 'vaccine' - ")
all_tweets_text = nltk.text.Text(all_words)
all_tweets_text.concordance("vaccine", width=150)

"""### Most common words"""

# Most common words by FreqDist
fdist = nltk.FreqDist(all_words)
print("Total words: ", len(all_words))
most_common = fdist.most_common(30)
print("Top 30 most common words - ", most_common)
plt.figure(figsize=(10, 6))
fdist.plot(30, cumulative=False)
plt.show()

# Tweets contining these words have been retweeted the most
tweet_tokenizer = TweetTokenizer()

## tokenize the tweets
tokenized_tweets = list(df.text.apply(lambda x: tweet_tokenizer.tokenize(x)))

## remove the stopwords and punctuations
stopwords_eng = nltk.corpus.stopwords.words('english')
punctuation = list(string.punctuation)
stop_words = stopwords_eng + punctuation
cleaned_words = [word for tweet in tokenized_tweets for word in tweet if not word in stop_words]

## retweet count dictionary
def retweet_counter(df):
  """
  This function takes in a dataframe
  Returns a dictionary of word:retweets
  """
  word_retweet_count = dict.fromkeys(cleaned_words, 0)
  for row in df.values:
    for word in word_retweet_count.keys():
      if word in row[0]:
        word_retweet_count[word] =+ row[1]
  return word_retweet_count

word_retweet_count = retweet_counter(df[['text', 'retweets']])

word_by_retweets = sorted(word_retweet_count.items(), key=lambda x: x[1], reverse=True)
word_by_retweets[:30]

"""This particular link has the highest number of retweets!"""

# Commented out IPython magic to ensure Python compatibility.
# %%html
# <iframe src="https://drive.google.com/file/d/19mQ1a6mxqsCTeu3SEAvzlMwOn3puE_Cf/preview" width="350" height="350"></iframe>
# </br>
# <p>Later, we will also see how the general sentiment is regarding the vaccine</p>

print("Words in bigrams occurring together more frequently than others - \nCollocations - ")
all_tweets_text.collocations(num=50)

"""### Location Analysis"""

locations = pd.DataFrame(df.groupby('user_location').count()).reset_index()[['user_location', 'id']].sort_values('id', ascending=False)
locations.columns = ['location', 'count']
top100_locations = locations.iloc[:100]
top100_locations

geocoder = Nominatim(user_agent='geocode_location')
geocode = RateLimiter(geocoder.geocode, min_delay_seconds=0.1, return_value_on_exception=None)

func = lambda loc: None if pd.isnull(loc) else (geocode(loc)[0].split(', ')[-1] if geocode(loc) else None)
countries = top100_locations['location'].apply(func)
countries

print("Top 100 user locations are from {0} countries".format(len(countries.unique())-1)) # excluding None

plt.figure(figsize=(20, 10))
ax = sns.barplot(x=countries.value_counts().index, y=countries.value_counts().values)
plt.xticks(rotation=70)
plt.title("Tweets by Country")
for p in ax.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height() + 0.5
  value = '{:.0f}'.format(p.get_height())
  ax.text(_x, _y, value, ha="center")
plt.grid()
plt.show()
print((countries.value_counts().index))

"""### Influence of verified users"""

# Influence of verified users
verified_users_flag = pd.Series(["Yes" if df['user_verified'].iloc[num]==True else "No" for num in range(len(df))])
verified_users_flag

"""#### Difference in how they tweet"""

source_comparision = pd.concat([verified_users_flag, df.source], axis=1)
source_comparision.columns = ['verified?', 'source']
source_comparision

com = source_comparision.groupby('source')['verified?'].value_counts()
com_df = pd.DataFrame(com)
com_df.columns=['count']
com_df = com_df.reset_index()

plt.figure(figsize=(15, 10))
ax2 = sns.barplot(data=com_df, x=com_df['count'], y=com_df['source'], hue=com_df['verified?'])
plt.legend(title="Verified?", loc='upper right')
plt.show()

"""#### From where are they tweeting?"""

location_comparision = pd.concat([verified_users_flag, df.user_location], axis=1)
location_comparision.columns = ['verified?', 'location']
location_comparision

"""Someone is tweeting from my bed...?

#### Popularity
"""

popularity = pd.concat([verified_users_flag, df.retweets, df.favorites], axis=1)
popularity.columns = ['verified?', 'retweets', 'favorites']
popularity

pop_per = (popularity['verified?'].value_counts()/len(popularity)*100).values
pop_per = [round(percent, 2) for percent in pop_per]

plt.figure(figsize=(6, 4))
ax = sns.barplot(x=['No', 'Yes'], y=pop_per)
plt.title("Percentage of verified v/s normal")
for p in ax.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height() + 1.5
  value = '{:.2f}%'.format(p.get_height())
  ax.text(_x, _y, value, ha="center") 

plt.show()

# Plotting retweets and favorites to see influentce of verfied users 

popularity_by_verification = popularity.groupby('verified?').sum().reset_index()
new_pop = pd.melt(popularity_by_verification, id_vars=['verified?'], var_name='type', value_name='count')
plt.figure(figsize=(10, 8))
ax = sns.barplot(data=new_pop, x='type', y='count', hue='verified?')
for p in ax.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height()
  value = '{:.0f}'.format(p.get_height())
  ax.text(_x, _y, value, ha="center") 

plt.title('Retweets and Favorites comparison')
plt.show()

"""Insights - 

*   Even though verified users form a mere ~10% of the user base, the engagemnet generated by them is comparable to the remaining 90% - specially in terms of retweets.
*   Why retweets are comparable and favorites not so much? - That is Twitter Psychology and social media strategy at play!
*   In terms of Twitter Psychology, favorites provide many ways for the followers to engage - they can save the tweet for later, or maybe they share the same sentiment and identify with the tweet and this is a way to show their support. (There are several shades of favorites gaining popularity like hate favorite weirdly enough) - [More](https://slate.com/technology/2014/06/twitter-favorite-button-researchers-find-25-distinct-reasons-for-its-use.html)
*   In terms of social media strategy, retweets matter more as they allow the original tweeter as well as the re-tweeter more visibility and coverage - hence more popularity and potentially more followers.
"""

retweets = df[df['retweets']>0].sort_values(by='retweets', ascending=False)
retweets.head(20)

"""### When are people tweeting?"""

datetime = df[['id', 'date']]
datetime.columns = ['id', 'datetime']
datetime['date'] = datetime['datetime'].apply(lambda datetime: datetime[:10])
dates = datetime.groupby('date').count()['id'].reset_index().sort_values('date', ascending=True)
dates.head()

dates.max()['id']

plt.figure(figsize=(18, 10))
ax = sns.lineplot(x='date', y='id', data=dates, marker="D")
ax2 = sns.barplot(x='date', y='id', data=dates, color='red')
plt.title('Tweet Frequency')
plt.xticks(rotation=90)
plt.ylabel('Frequency')
plt.xlabel('Date')

# labelling the points
for x, y in zip(dates['date'], dates['id']):
  plt.text(x = x, y = y+5, s = "{:.0f}".format(y), color = 'white', rotation=45).set_backgroundcolor('#965786')

for bar in ax2.patches:
  bar.set_x(bar.get_x() + 0.4)
  bar.set_width(0.1)
   
plt.show()

tweets_08_09 = list(df[(df['date'].str.contains('2021-01-08') | df['date'].str.contains('2021-01-09'))]['text'])
tweets_08_09[:5]

tweets_08_09_text = " ".join(tweets_08_09)
print ("There are {} words in the combination of all review.".format(len(tweets_08_09_text)))

# Generate a word cloud image
wordcloud = WordCloud(stopwords=set(STOPWORDS), background_color="white").generate(tweets_08_09_text)

# Display the generated image:
# the matplotlib way:
plt.figure(figsize=(10, 15))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Most common words by FreqDist
fdist_08_09 = nltk.FreqDist([word for word in tweets_08_09_text.split(" ")])
most_common_08_09 = fdist_08_09.most_common(30)
print("Top 30 most common words on 08-01-2021- ")
most_common_08_09

"""## Analyzing Sentiments

VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. VADER uses a combination of A sentiment lexicon is a list of lexical features (e.g., words) which are generally labelled according to their semantic orientation as either positive or negative.

### Without Data Preprocessing
"""

## VADER Sentiment Analyzer
analyser = vader.SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(tweet):
  score = analyser.polarity_scores(tweet) 
  return analyser.polarity_scores(tweet)

df['sentiment_scores'] = df['text'].apply(sentiment_analyzer_scores)
df.head(10)

df.text.iloc[0], df.sentiment_scores.iloc[0]

df.sentiment_scores.iloc[0].keys(), df.sentiment_scores.iloc[0].values()

df['sentiment_scores_pos'] = df['sentiment_scores'].apply(lambda score: score['pos'])
df['sentiment_scores_neu'] = df['sentiment_scores'].apply(lambda score: score['neu'])
df['sentiment_scores_neg'] = df['sentiment_scores'].apply(lambda score: score['neg'])

df['sentiment'] = df['sentiment_scores'].apply(lambda score: list(score.keys())[list(score.values()).index(max(score['pos'], score['neu'], score['neg']))])

df.head()

sentiments = df.groupby('sentiment').count()[['id']].sort_values('id', ascending=False)
ax = plt.figure(figsize=(5, 3), facecolor='lightgrey')

sentiments_plot = sns.barplot(data=sentiments, x=sentiments.index, y='id')
plt.xticks(rotation=75)

for p in sentiments_plot.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height() + 1
  value = '{:.0f}'.format(p.get_height())
  sentiments_plot.text(_x, _y, value, ha="center") 

plt.grid()
plt.title("Twitter Sentiment Analysis - Without Data Preprocessing")
plt.xlabel("Sentiment")
plt.ylabel("Frequency")
plt.show()

"""### With Data Preprocessing"""

df.columns

## lower tweet
def lower_tweet(tweet):
  return tweet.lower()

## convert emojis
def convert_emojis(tweet):
  demojized_tweet = emoji.demojize(tweet)
  return demojized_tweet

## remove emojis
def remove_emojis(tweet):
  emoj = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002500-\U00002BEF"  # chinese char
        u"\U00002702-\U000027B0"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U00010000-\U0010ffff"
        u"\u2640-\u2642" 
        u"\u2600-\u2B55"
        u"\u200d"
        u"\u23cf"
        u"\u23e9"
        u"\u231a"
        u"\ufe0f"  # dingbats
        u"\u3030"
        "]+", re.UNICODE)
  return re.sub(emoj, '', tweet)

## tokenize tweet
def tweet_tokenize(tweet):
  tokenized_tweet = tweet_tokenizer.tokenize(tweet)
  return tokenized_tweet

## remove stopwords
def remove_stopwords(tweet):
  english_stopwords = stopwords.words('english')
  cleaned_tweet = [word for word in tweet if not word in english_stopwords]
  return cleaned_tweet

## lemmatize tweet
def lemmatize_tweet(tweet):
  wordnet_lemmatizer = WordNetLemmatizer()
  lemmas = [wordnet_lemmatizer.lemmatize(word) for word in tweet]
  return ' '.join(lemmas)

print(remove_stopwords(tweet_tokenize(lower_tweet(df.text.iloc[0]))))
data_preprocessing_pipeline.transform(df.text.iloc[0])

# Data Preprocessing Pipeline
data_preprocessing_pipeline = Pipeline(steps=[
                                      ('lower_tweet', FunctionTransformer(lower_tweet)), 
                                      ('tweet_tokenize', FunctionTransformer(tweet_tokenize)), 
                                      ('remove_stopwords', FunctionTransformer(remove_stopwords)), 
                                      ('lemmatize_tweet', FunctionTransformer(lemmatize_tweet))])

df['preprocessed_tweets'] = df.text.apply(lambda tweet: data_preprocessing_pipeline.transform(tweet))

df.head()

## VADER Sentiment Analyzer
analyser = vader.SentimentIntensityAnalyzer()

def sentiment_analyzer_scores(tweet):
  score = analyser.polarity_scores(tweet) 
  return analyser.polarity_scores(tweet)

df['new_sentiment_scores'] = df['preprocessed_tweets'].apply(sentiment_analyzer_scores)
df.head(10)

df['new_sentiment_scores_pos'] = df['new_sentiment_scores'].apply(lambda score: score['pos'])
df['new_sentiment_scores_neu'] = df['new_sentiment_scores'].apply(lambda score: score['neu'])
df['new_sentiment_scores_neg'] = df['new_sentiment_scores'].apply(lambda score: score['neg'])

df['new_sentiment'] = df['new_sentiment_scores'].apply(lambda score: list(score.keys())[list(score.values()).index(max(score['pos'], score['neu'], score['neg']))])

sentiments = df.groupby('new_sentiment').count()[['id']].sort_values('id', ascending=False)
ax = plt.figure(figsize=(5, 3), facecolor='lightgrey')

sentiments_plot = sns.barplot(data=sentiments, x=sentiments.index, y='id')
plt.xticks(rotation=75)

for p in sentiments_plot.patches:
  _x = p.get_x() + p.get_width() / 2
  _y = p.get_y() + p.get_height() + 1
  value = '{:.0f}'.format(p.get_height())
  sentiments_plot.text(_x, _y, value, ha="center") 

plt.grid()
plt.title("Twitter Sentiment Analysis - With Data Preprocessing")
plt.xlabel("Sentiment")
plt.ylabel("Frequency")
plt.show()

"""## VADER CITATION

VADER CITATION </br>
Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.

# Installing Packages
"""

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download("vader_lexicon")

!pip install emot
!pip install nbconvert
!sudo apt-get install texlive-xetex texlive-fonts-recommended texlive-generic-recommended

!jupyter nbconvert --to pdf "/content/drive/MyDrive/Colab Notebooks/Pfizer Vaccine Tweets/pfizer_twitter_analysis.ipynb"

!pip freeze > requirements.txt

